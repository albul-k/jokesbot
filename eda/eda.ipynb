{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "gb_NLP_cv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.7 64-bit ('venv': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "interpreter": {
      "hash": "de869d8548fe524b73cd4304b0ac9362679f963e689f7a801fb360cbfb92d63f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to NLP\r\n",
        "\r\n",
        "## Course work"
      ],
      "metadata": {
        "id": "d33r2odztuUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install pymorphy2[fast] annoy stop_words transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2[fast]\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 81 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 102 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 122 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 143 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 646 kB 32.1 MB/s \n",
            "\u001b[?25hCollecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 31.9 MB/s \n",
            "\u001b[?25hCollecting DAWG>=0.8\n",
            "  Downloading DAWG-0.8.0.tar.gz (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Building wheels for collected packages: annoy, stop-words, DAWG\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391658 sha256=06fa92db742929978b2b2ff230ca95c86af08164ed7a6b695bc675c78cf5e652\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32912 sha256=a18c086b6c86cd800cbc55c7da7107fae19f683f724fc0011f70fbcf051928d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=853121 sha256=75afbba128784526982692e1d8d7312e2c0325caefd9b9260d9689fd5e875ed3\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/51/a4/2de41ff197786537075027c27b479a38da92f50abc86634445\n",
            "Successfully built annoy stop-words DAWG\n",
            "Installing collected packages: pyyaml, pymorphy2-dicts-ru, dawg-python, tokenizers, sacremoses, pymorphy2, huggingface-hub, DAWG, transformers, stop-words, annoy\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed DAWG-0.8.0 annoy-1.17.0 dawg-python-0.7.2 huggingface-hub-0.0.19 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pyyaml-6.0 sacremoses-0.0.46 stop-words-2018.7.23 tokenizers-0.10.3 transformers-4.12.2\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mFizo3bytuUd",
        "outputId": "4cffcf19-183a-4713-e23e-a58656f03b1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import string\r\n",
        "import nltk\r\n",
        "import annoy\r\n",
        "import numpy as np\r\n",
        "from pymorphy2 import MorphAnalyzer\r\n",
        "from stop_words import get_stop_words\r\n",
        "from gensim.models import FastText\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIWwjADEtuUg",
        "outputId": "0a431288-6712-46bd-e112-1eee9065daa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')\r\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/data/gb_NLP_cv/'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1UiAGaZfJfy",
        "outputId": "1bc01874-b8ae-48d7-d8b1-78f7ff4404f6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "morpher = MorphAnalyzer()\r\n",
        "sw = set(get_stop_words(\"ru\") + nltk.corpus.stopwords.words('russian'))\r\n",
        "exclude = set(string.punctuation)\r\n",
        "\r\n",
        "def preprocess_txt(line):\r\n",
        "  spls = \"\".join(i for i in line.strip() if i not in exclude).split()\r\n",
        "  spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\r\n",
        "  spls = [i for i in spls if i not in sw and i != \"\"]\r\n",
        "  return ' '.join(spls)"
      ],
      "outputs": [],
      "metadata": {
        "id": "s1ok44cJtuUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching database"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "import sqlite3 as sql\r\n",
        "\r\n",
        "\r\n",
        "cur = sql.connect(f'{path}jokes.db').execute('SELECT theme,text FROM joke')\r\n",
        "rows = cur.fetchall()\r\n",
        "cur.close()\r\n",
        "\r\n",
        "jokes = {idx: {'theme': itm[0], 'text': itm[1]} for idx,itm in enumerate(rows)}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-02 16:15:38,984 INFO sqlalchemy.engine.Engine SELECT theme,text FROM joke\n",
            "2021-11-02 16:15:38,985 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYxVPuMKf4mY",
        "outputId": "d8f86981-83ea-403e-9a35-e82fc8d78df5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "print(f'Theme: {jokes[0][\"theme\"]}\\n')\r\n",
        "print(f'Text: {jokes[0][\"text\"]}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme: pro-sudey\n",
            "\n",
            "Text: На суде в Стамбуле обвиняемый сказал:\r\n",
            "- На свои жертвы я нападал всегда днем. Ночью я бы побоялся\r\n",
            "ходить с награбленными деньгами...\r\n",
            "\r\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alEtISc32Fhk",
        "outputId": "4ecde5f2-c627-47cf-deba-defe5ecec15f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the text"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "df = pd.DataFrame.from_dict(jokes, columns=['theme', 'text'], orient='index')\r\n",
        "df['text_token'] = df['text'].apply(lambda x: preprocess_txt(x))\r\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>theme</th>\n",
              "      <th>text</th>\n",
              "      <th>text_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pro-sudey</td>\n",
              "      <td>На суде в Стамбуле обвиняемый сказал:\\r\\n- На...</td>\n",
              "      <td>суд стамбул обвиняемый свой жертва нападать н...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pro-sudey</td>\n",
              "      <td>- Вы продолжаете утверждать, что обвиняемый н...</td>\n",
              "      <td>продолжать утверждать обвиняемый назвать дура...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pro-sudey</td>\n",
              "      <td>На суде.\\r\\n- Итак, когда дело дошло до столкн...</td>\n",
              "      <td>суд итак дело дойти столкновение противник рук...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pro-sudey</td>\n",
              "      <td>Старую леди сбил автомобиль. На суде ее спраши...</td>\n",
              "      <td>старый леди сбить автомобиль суд спрашивать де...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pro-sudey</td>\n",
              "      <td>Судья говорит:\\r\\n- Согласно вашей жалобе, об...</td>\n",
              "      <td>судья говорить согласно вашей жалоба обвиняем...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       theme  ...                                         text_token\n",
              "0  pro-sudey  ...  суд стамбул обвиняемый свой жертва нападать н...\n",
              "1  pro-sudey  ...  продолжать утверждать обвиняемый назвать дура...\n",
              "2  pro-sudey  ...  суд итак дело дойти столкновение противник рук...\n",
              "3  pro-sudey  ...  старый леди сбить автомобиль суд спрашивать де...\n",
              "4  pro-sudey  ...  судья говорить согласно вашей жалоба обвиняем...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "pThvzMnuMzYV",
        "outputId": "e921e8a2-0b04-4853-926b-516767b1ddb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "print(df.loc[0, 'text'])\r\n",
        "print(df.loc[0, 'text_token'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "На суде в Стамбуле обвиняемый сказал:\r\n",
            "- На свои жертвы я нападал всегда днем. Ночью я бы побоялся\r\n",
            "ходить с награбленными деньгами...\r\n",
            "\r\n",
            "\n",
            "суд стамбул обвиняемый свой жертва нападать ночью побояться ходить награбить деньга\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGyMuJXRNlfN",
        "outputId": "edf935bf-1940-48a8-b6ca-394ecb1d65a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target feature encoding"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "le = LabelEncoder()\r\n",
        "df['theme'] = le.fit_transform(df['theme'])\r\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>theme</th>\n",
              "      <th>text</th>\n",
              "      <th>text_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34</td>\n",
              "      <td>На суде в Стамбуле обвиняемый сказал:\\r\\n- На...</td>\n",
              "      <td>суд стамбул обвиняемый свой жертва нападать н...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34</td>\n",
              "      <td>- Вы продолжаете утверждать, что обвиняемый н...</td>\n",
              "      <td>продолжать утверждать обвиняемый назвать дура...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34</td>\n",
              "      <td>На суде.\\r\\n- Итак, когда дело дошло до столкн...</td>\n",
              "      <td>суд итак дело дойти столкновение противник рук...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34</td>\n",
              "      <td>Старую леди сбил автомобиль. На суде ее спраши...</td>\n",
              "      <td>старый леди сбить автомобиль суд спрашивать де...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34</td>\n",
              "      <td>Судья говорит:\\r\\n- Согласно вашей жалобе, об...</td>\n",
              "      <td>судья говорить согласно вашей жалоба обвиняем...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   theme  ...                                         text_token\n",
              "0     34  ...  суд стамбул обвиняемый свой жертва нападать н...\n",
              "1     34  ...  продолжать утверждать обвиняемый назвать дура...\n",
              "2     34  ...  суд итак дело дойти столкновение противник рук...\n",
              "3     34  ...  старый леди сбить автомобиль суд спрашивать де...\n",
              "4     34  ...  судья говорить согласно вашей жалоба обвиняем...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EiCj109sO-pI",
        "outputId": "e2300831-5605-48a2-f5cc-1da3a9eb5c34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "with open(os.path.join(path, 'label_encoder.pkl'), 'wb') as file:\r\n",
        "  pickle.dump(le, file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "eBsSB4Jn0wVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SVC model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "  df['text_token'], df['theme'],\r\n",
        "  test_size=0.2,\r\n",
        "  stratify=df['theme'],\r\n",
        "  random_state=100\r\n",
        ")\r\n",
        "\r\n",
        "tfidf = TfidfVectorizer(\r\n",
        "  ngram_range=(1, 2),\r\n",
        "  max_features=10000\r\n",
        ")\r\n",
        "\r\n",
        "X_train_idf = tfidf.fit_transform(X_train)\r\n",
        "X_test_idf = tfidf.transform(X_test)\r\n",
        "\r\n",
        "svc = LinearSVC(\r\n",
        "    random_state=100,\r\n",
        "    max_iter=1000,\r\n",
        "    loss='squared_hinge',\r\n",
        "    dual=False,\r\n",
        ")\r\n",
        "svc.fit(X_train_idf, y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=100, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clacvKMyInXi",
        "outputId": "0a05d06c-4b47-42b1-c4f4-9df2ef065591"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "\r\n",
        "def calc_score(model, X, y):\r\n",
        "  cv = cross_val_score(\r\n",
        "      model,\r\n",
        "      X, y,\r\n",
        "      cv=5,\r\n",
        "      scoring='f1_weighted'\r\n",
        "  )\r\n",
        "  print(f'F1_weighted cv: {np.mean(cv)}')\r\n",
        "  print(f'\\nClassification report\\n{classification_report(y, model.predict(X))}')\r\n",
        "\r\n",
        "calc_score(svc, X_test_idf, y_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1_weighted cv: 0.5459447624943581\n",
            "\n",
            "Classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.82      0.74      7106\n",
            "           1       0.22      0.02      0.04       104\n",
            "           2       0.33      0.10      0.15        94\n",
            "           3       0.50      0.09      0.15        67\n",
            "           4       0.41      0.21      0.28       398\n",
            "           5       0.30      0.09      0.14       288\n",
            "           6       0.00      0.00      0.00         2\n",
            "           7       0.00      0.00      0.00         4\n",
            "           8       0.28      0.05      0.09       430\n",
            "           9       0.95      0.83      0.89       230\n",
            "          10       0.63      0.47      0.54       454\n",
            "          11       0.78      0.91      0.84        23\n",
            "          12       0.97      0.66      0.78        47\n",
            "          13       0.83      0.89      0.86        91\n",
            "          14       0.89      0.85      0.87       419\n",
            "          15       0.00      0.00      0.00         3\n",
            "          16       0.77      0.62      0.68       206\n",
            "          17       0.00      0.00      0.00        47\n",
            "          18       0.29      0.05      0.08       110\n",
            "          19       0.25      0.04      0.06       108\n",
            "          20       0.92      0.92      0.92       124\n",
            "          21       0.06      0.01      0.02       116\n",
            "          22       0.98      0.69      0.81       573\n",
            "          23       0.11      0.01      0.02       380\n",
            "          24       0.20      0.03      0.05       197\n",
            "          25       0.53      0.47      0.50       207\n",
            "          26       0.60      0.56      0.58        59\n",
            "          27       1.00      0.86      0.93       152\n",
            "          28       0.95      0.85      0.90        86\n",
            "          29       0.45      0.35      0.39      1159\n",
            "          30       0.98      0.96      0.97       193\n",
            "          31       0.00      0.00      0.00         4\n",
            "          32       0.39      0.21      0.27       118\n",
            "          33       0.50      0.34      0.41       293\n",
            "          34       0.87      0.89      0.88        73\n",
            "          35       0.86      0.94      0.90       148\n",
            "          36       0.91      0.86      0.88       265\n",
            "          37       0.79      0.65      0.71        34\n",
            "          38       0.55      0.66      0.60      8398\n",
            "          39       0.95      0.82      0.88       322\n",
            "          40       0.47      0.08      0.14       101\n",
            "          41       0.70      0.25      0.37       112\n",
            "          42       0.75      0.16      0.26        19\n",
            "          43       0.68      0.60      0.64      2677\n",
            "\n",
            "    accuracy                           0.63     26041\n",
            "   macro avg       0.55      0.43      0.46     26041\n",
            "weighted avg       0.61      0.63      0.61     26041\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuxpOMHRTTxY",
        "outputId": "81ad9a61-572e-487c-c5af-800ee13ffa31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train FastText model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "class MyIter:\r\n",
        "  def __iter__(self):\r\n",
        "    for index, row in df.iterrows():\r\n",
        "      yield row['text_token']\r\n",
        "\r\n",
        "\r\n",
        "modelFT = FastText(sentences=MyIter(), size=30, min_count=1, window=5, workers=8)\r\n",
        "modelFT.save(os.path.join(path, 'model_FT.ft'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "XYH9auj7bTTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "ft_index = annoy.AnnoyIndex(30 ,'angular')\r\n",
        "\r\n",
        "idfs = {v[0]: v[1] for v in zip(tfidf.vocabulary_, tfidf.idf_)}\r\n",
        "midf = np.mean(tfidf.idf_)\r\n",
        "\r\n",
        "index_map = {}\r\n",
        "counter = 0\r\n",
        "\r\n",
        "for index, row in df.iterrows():\r\n",
        "    n_ft = 0\r\n",
        "    index_map[counter] = (df.loc[index, \"theme\"], df.loc[index, \"text\"], df.loc[index, \"text_token\"])\r\n",
        "    vector_ft = np.zeros(30)\r\n",
        "    for word in df.loc[index, \"text_token\"]:\r\n",
        "        if word in modelFT.wv:\r\n",
        "            vector_ft += modelFT.wv[word] * idfs.get(word, midf)\r\n",
        "            n_ft += idfs.get(word, midf)\r\n",
        "    if n_ft > 0:\r\n",
        "        vector_ft = vector_ft / n_ft\r\n",
        "    ft_index.add_item(counter, vector_ft)\r\n",
        "    counter += 1\r\n",
        "\r\n",
        "ft_index.build(10)\r\n",
        "ft_index.save(os.path.join(path, 'ft_index.ann'))\r\n",
        "\r\n",
        "with open(os.path.join(path, 'index_map.pkl'), 'wb') as file:\r\n",
        "  pickle.dump(index_map, file)\r\n",
        "\r\n",
        "with open(os.path.join(path, 'tfidf.pkl'), 'wb') as file:\r\n",
        "  pickle.dump(tfidf, file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "O15KzJHLUXuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "def embed_txt(txt, idfs, midf):\r\n",
        "  n_ft = 0\r\n",
        "  vector_ft = np.zeros(30)\r\n",
        "  for word in txt:\r\n",
        "    if word in modelFT.wv:\r\n",
        "      vector_ft += modelFT.wv[word] * idfs.get(word, midf)\r\n",
        "      n_ft += idfs.get(word, midf)\r\n",
        "  return vector_ft / n_ft\r\n",
        "\r\n",
        "def get_predictions(text):\r\n",
        "  text = preprocess_txt(text)\r\n",
        "  text_ = tfidf.transform([text])\r\n",
        "  theme = svc.predict(text_)\r\n",
        "\r\n",
        "  vect_ft = embed_txt(text, idfs, midf)\r\n",
        "  ft_index_val, distances = ft_index.get_nns_by_vector(vect_ft, 10, include_distances=True)\r\n",
        "  joke = None\r\n",
        "  for item, dist in zip(ft_index_val, distances):\r\n",
        "    if dist <= 0.25:\r\n",
        "      joke = index_map[item][1]\r\n",
        "    break\r\n",
        "  if joke:\r\n",
        "    return joke\r\n",
        "  \r\n",
        "  joke = df.loc[df[\"theme\"] == theme[0]].sample()['text'].values[0]\r\n",
        "  return joke"
      ],
      "outputs": [],
      "metadata": {
        "id": "VtCOTB0Xc2yc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "for word in ['путин', 'деньги', 'чукча', 'штирлиц', 'мент', 'про рабиновича', 'женщина', 'алкоголик', 'судья', 'чукча', 'про поручика']:\r\n",
        "  print(f'word={word}')\r\n",
        "  print(get_predictions(word))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "путин\n",
            "Из сводок новостей: Борис Николаевич Ельцин неожиданно прервал свой отпуск и вылетел в Москву. Зайдя в спальню, он обнаружил там: Наину Иосифовну и некоего мужчину (в шкафу) , для которых все это было также неожиданно. Мужчиной оказался премьер правительства Российской Федерации Владимир Владимирович Путин, с которым Борис Николаевич Ельцин срочно провел совещание.\r\n",
            "\n",
            "деньги\n",
            "Мы всю жизнь бегаем за деньгами, а деньги от нас.\r\n",
            "\r\n",
            "\r\n",
            "\n",
            "чукча\n",
            "Чукча подходит к проститутке на Тверской:\r\n",
            "- Скока за ночь, однака?\r\n",
            "Она смерила его взглядом:\r\n",
            "- С тебя 150 баксов, лох.\r\n",
            "Тут подскакивает чукотская братва и запихивает ее в багажник. Прошло полгода... ее высаживают на том же месте, одичавшую, вонючую, в унтах и чукотских тряпках: в школе она училась плохо, откуда ей было знать, что ночь полярная.\r\n",
            "\n",
            "штирлиц\n",
            "Штирлиц ехал по шоссе. Вдруг он увидел голосующего Мюллера.\r\n",
            "- Не могу я подвозить этого палача, погубившего миллионы советских\r\n",
            "людей! - подумал Штирлиц и проехал мимо.\r\n",
            "Через минуту он опять увидел на шоссе голосующего Мюллера.\r\n",
            "- Ни за что не возьму этого двурушника и резонера, - подумал Штирлиц\r\n",
            "и проехал мимо.\r\n",
            "Еще через минуту он опять увидел на шоссе голосующего Мюллера.\r\n",
            "- Очевидно, кольцевая... - подумал Штирлиц.\r\n",
            "- Издевается... - подумал Мюллер.\r\n",
            "\r\n",
            "\n",
            "мент\n",
            "год мента\r\n",
            "\r\n",
            "\r\n",
            "\n",
            "про рабиновича\n",
            "- Алло, господин Рабинович?- Да, а шо такое?- Это вас из банка звонят, у вас отрицательный баланс, - 500$!- Да? Шо вы говорите? А неделю назад посмотрите сколько было?- +500.- А две недели назад?- +1000.- А три недели?- +1500.- И что, я вам хоть раз позвонил?\r\n",
            "\n",
            "женщина\n",
            "Он хотел от женщин одного. Они - всего...\r\n",
            "\r\n",
            "\r\n",
            "\n",
            "алкоголик\n",
            "Пьяный мужик поймал интеллигентного мальчика и говорит:\r\n",
            "-Ругайся матом!\r\n",
            "-Не умею.\r\n",
            "-Ругайся как умеешь!\r\n",
            "-П*п*ска!\r\n",
            "-Так мало?..\r\n",
            "-ВАГОН П*П*СЕК!!!\r\n",
            "\r\n",
            "\n",
            "судья\n",
            "Судья: - Гоги, ты зачем мальчика изнасиловал? Гоги: - Иду я по лесу. Вижу - мальчик. Думал, дикий...\r\n",
            "\r\n",
            "\n",
            "чукча\n",
            "Однажды Чукча стоит на посту. Кто-то идет мимо. Чукча:\r\n",
            "\"Пароля!\"  - \"Пошел на ...!\" Чукча: \"Однако, сколько лет стою, а пароль\r\n",
            "не меняют.\"\r\n",
            "\r\n",
            "\n",
            "про поручика\n",
            "- Поручик, ваш член как сибирский экспресс.\r\n",
            "- Что-с, такой длинный?\r\n",
            "- Да нет стоит всего одну минуту.\r\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5TCGrhWdVZi",
        "outputId": "32b1e189-5de3-4cd4-b305-01480623097d"
      }
    }
  ]
}